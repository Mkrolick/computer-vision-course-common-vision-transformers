# Dilated Neighborhood Attention Transformer (DINAT)

![DINAT Architecture Diagram](https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/dinat_images/dina_comparison.png)
## Overview of Architecture

The Dilated Neighborhood Attention Transformer (DiNAT) is a vision transformer that builds upon the Neighborhood Attention (NA) mechanism. DiNA extends NA's local attention by introducing a sparse global attention that captures more extensive global context without additional computational overhead. This combination powers DiNAT to maintain locality, preserve translational equivariance, and exponentially expand the receptive field. DiNAT's architecture gradually changes dilation through the model, optimizing the extension of receptive fields for effective feature learning. With these enhancements, DiNAT can outperform strong baseline architectures such as ConvNeXt.

## Core of DiNAT: Neighborhood Attention

DINAT is based on the Neighborhood Attention (NA) architecture which is an attention mechanism specifically designed for computer vision tasks, aiming to efficiently capture relationships between pixels in an image. In a simple analogy, imagine you have an image, and each pixel in that image needs to understand and focus on its nearby pixels to make sense of the entire picture. Let's examine the key features of NA:

* **Local Relationships**: NA captures local relationships, allowing each pixel to consider information from its immediate surroundings. This is similar to how we might understand a scene by looking at the objects closest to us first before considering the entire view.

* **Efficiency**: NA is designed to be efficient in processing images. It's like giving each pixel a small window or scope to look around, rather than having it examine the entire picture. This helps in reducing computational complexity and memory usage, making it more feasible for large-scale image processing tasks.

* **Receptive Field**: NA allows pixels to grow their understanding of their environment without needing too much extra computation. It dynamically expands their scope or "attention span" to include more distant neighbors when necessary.

In essence Neighborhood Attention is a technique that enables pixels in an image to focus on their immediate surroundings, helping them understand local relationships efficiently. This localized understanding contributes to building a detailed understanding of the entire image while managing computational resources effectively.

## The Evolution of DINAT 

The Evolution of the Dilated Neighborhood Attention Transformer represents a significant improvement in hierarchical vision transformers. It addresses the limitations of existing attention mechanisms. Initially, Neighborhood Attention was introduced to provide locality and efficiency, but it fell short in capturing global context. To overcome this limitation, the concept of Dilated Neighborhood Attention (DiNA) was introduced. DiNA extends the NA by expanding neighborhoods into larger sparse regions. This allows for the capture of more global context and exponentially increases the receptive field without adding computational burden. The next development is DiNAT, which combines localized NA with the expanded global context of DiNA. DiNAT achieves this by gradually changing dilation throughout the model, optimizing receptive fields, and simplifying feature learning.

![DINAT Architecture Diagram](https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/dinat_images/dinat_architecture.png)

## Key Features of DiNAT:

* **Expanding Receptive Field**: DiNAT extends the receptive field exponentially while maintaining linear computational complexity. DiNA spans neighborhoods over longer ranges by increasing the step size which is crucial for understanding complex visual scenes and relationships between distant elements in an image.

*  **Preserving Locality and Symmetry**: DiNAT works on NA to ensure the preservation of locality while using sparse global attention. The local attention of NA maintains the model's ability to focus on nearby features while DiNAT's sparse global attention allows for the integration of distant information to create a balance between local and global context understanding.

* **Scalability and Adaptability**: DiNAT's gradual dilation change through the model enhances its adaptability to varying resolutions without the need for significantly altering window sizes. This scalability is essential for handling diverse image sizes or resolutions encountered in real-world applications.

## Image Classification with DINAT

```python
from transformers import AutoImageProcessor, DinatForImageClassification
from PIL import Image
import requests

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = AutoImageProcessor.from_pretrained("shi-labs/dinat-mini-in1k-224")
model = DinatForImageClassification.from_pretrained("shi-labs/dinat-mini-in1k-224")

inputs = feature_extractor(images=image, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits
# model predicts one of the 1000 ImageNet classes
predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", model.config.id2label[predicted_class_idx])

```


## OneFormer: Universal Image Segmentation Framework
![One Former](https://praeclarumjj3.github.io/oneformer/text_gen.svg)


One of the use cases of DINAT transformer is OneFormer which is an innovative framework designed for image segmentation. The goal of OneFormer is to create a single, versatile model capable of performing multiple types of image segmentation tasks without needing separate models or extensive training on each task.

Traditionally, different types of image segmentation tasks like semantic segmentation, instance segmentation, and panoptic segmentation required specialized models and training procedures. What makes OneFormer unique is its ability to handle all these tasks using a single model. It achieves this by introducing a novel approach called "task-conditioned joint training." This approach allows the model to learn and excel at different segmentation tasks like identifying objects or their boundaries within the same training process. 

Additionally, OneFormer uses a method called query-text contrastive learning to help the model understand the differences between different segmentation tasks. This helps the model make more accurate predictions for each task.



## References
[1] [DINAT Paper](https://arxiv.org/abs/2209.15001)  
[2] [Hugging Face DINAT Transformer](https://huggingface.co/docs/transformers/model_doc/dinat)  
[3] [Neighborhood Attention(NA)](https://arxiv.org/abs/2204.07143)  
[4] [SHI Labs](https://huggingface.co/shi-labs)   
[5] [OneFormer Paper](https://arxiv.org/abs/2211.06220)  
[6] [Hugging Face OneFormer](https://huggingface.co/docs/transformers/main/en/model_doc/oneformer)  